{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-for-tf2\n",
      "  Using cached bert_for_tf2-0.14.9-py3-none-any.whl\n",
      "Collecting params-flow>=0.8.0 (from bert-for-tf2)\n",
      "  Using cached params_flow-0.8.2-py3-none-any.whl\n",
      "Collecting py-params>=0.9.6 (from bert-for-tf2)\n",
      "  Using cached py_params-0.10.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /home/halozhan/Study/.venv/lib/python3.10/site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.26.4)\n",
      "Requirement already satisfied: tqdm in /home/halozhan/Study/.venv/lib/python3.10/site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.67.1)\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
      "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow-hub\n",
      "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow-hub) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow-hub) (3.20.3)\n",
      "Collecting tf-keras>=2.14.1\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow<2.19,>=2.18 in /home/halozhan/.local/lib/python3.10/site-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.18.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (4.12.2)\n",
      "Requirement already satisfied: packaging in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (24.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.4.1)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.6.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.18.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.68.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.1.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (24.3.25)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (59.6.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.4.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.12.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.37.1)\n",
      "Requirement already satisfied: namex in /home/halozhan/.local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.0.8)\n",
      "Requirement already satisfied: rich in /home/halozhan/.local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (13.9.4)\n",
      "Requirement already satisfied: optree in /home/halozhan/.local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.13.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/halozhan/.local/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/halozhan/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.0.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/halozhan/.local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/halozhan/.local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/halozhan/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.1.2)\n",
      "Installing collected packages: tf-keras, tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.16.1 tf-keras-2.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 03:38:26.759721: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2024-12-03 03:38:26.759755: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 코드 10-41 라이브러리 호출 및 데이터셋 준비\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "import pandas as pd\n",
    "\n",
    "movie_reviews = pd.read_csv(\"data/IMDB Dataset.csv\")\n",
    "# 데이터셋(movie_reviews)에서 어떤 항목이 NaN을 가지고 있는지 확인\n",
    "movie_reviews.isnull().values.any()\n",
    "movie_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['review' 'sentiment']\n"
     ]
    }
   ],
   "source": [
    "# 코드 10-42 데이터셋 전처리\n",
    "import re\n",
    "\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    sentence = remove_tags(sen)  # html 태그 삭제\n",
    "    # 구두점(punctuation) 및 숫자(number) 제거, 문자(a~z, A~Z)가 아닌 것 제거\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", sentence)  # 단일 문자 제거(예 a)\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)  # 두 개 이상의 공백 제거\n",
    "    return sentence\n",
    "\n",
    "\n",
    "TAG_RE = re.compile(r\"<[^>]+>\")  # 정규 표현식(<[^>]+>)을 컴파일\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub(\"\", text)\n",
    "\n",
    "\n",
    "reviews = []\n",
    "sentences = list(movie_reviews[\"review\"])\n",
    "for sen in sentences:\n",
    "    # 모든 텍스트 리뷰 데이터를 preprocess_text 함수에 적용\n",
    "    reviews.append(preprocess_text(sen))\n",
    "\n",
    "print(movie_reviews.columns.values)  # 데이터셋의 열에 대한 이름 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 코드 10-43 sentiment 열에 대한 고윳값 확인\n",
    "movie_reviews.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-44 긍정/부정 감정 변환\n",
    "import numpy as np\n",
    "y = movie_reviews[\"sentiment\"]\n",
    "y = np.array(list(map(lambda x: 1 if x == \"positive\" else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phil the Alien is one of those quirky films where the humour is based around the oddness of everything rather than actual punchlines At first it was very odd and pretty funny but as the movie progressed didn find the jokes or oddness funny anymore Its low budget film thats never problem in itself there were some pretty interesting characters but eventually just lost interest imagine this film would appeal to stoner who is currently partaking For something similar but better try Brother from another planet \n"
     ]
    }
   ],
   "source": [
    "# 코드 10-45 리뷰 출력\n",
    "print(reviews[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# 코드 10-46 긍정/부정 리뷰 확인\n",
    "print(y[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 03:38:33.446700: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2024-12-03 03:38:33.596269: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-03 03:38:33.596298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:0a:00.0 name: NVIDIA GeForce GTX 1080 computeCapability: 6.1\n",
      "coreClock: 1.759GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\n",
      "2024-12-03 03:38:33.596351: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2024-12-03 03:38:33.596381: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\n",
      "2024-12-03 03:38:33.596407: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2024-12-03 03:38:33.596432: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2024-12-03 03:38:33.596456: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
      "2024-12-03 03:38:33.596480: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\n",
      "2024-12-03 03:38:33.596504: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-03 03:38:33.596508: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-12-03 03:38:33.596832: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 03:38:33.600777: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3800105000 Hz\n",
      "2024-12-03 03:38:33.602117: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x218ebf70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-12-03 03:38:33.602127: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-12-03 03:38:33.603128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-12-03 03:38:33.603136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      \n"
     ]
    }
   ],
   "source": [
    "# 코드 10-47 텍스트의 토큰화\n",
    "# bert.bert_tokenization 모듈의 FullTokenizer 클래스를 사용하여 객체를 만듭니다.\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "    trainable=False,\n",
    ")\n",
    "# 넘파이 배열 형식의 BERT 어휘 파일을 만듭니다.\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "# 텍스트를 소문자로 설정합니다.\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "# vocabulary_file 및 to_lower_case 변수를 BertTokenizer 객체에 전달합니다.\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don', \"'\", 't', 'be', 'so', 'judgment', '##al']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 코드 10-48 임의의 문장 토큰화\n",
    "tokenizer.tokenize(\"don't be so judgmental\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2123, 1005, 1056, 2022, 2061, 8689, 2389]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 코드 10-49 토큰의 ID 반환\n",
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"don't be so judgmental\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-50 리뷰 텍스트 데이터 토큰화\n",
    "def tokenize_reviews(text_reviews):\n",
    "    # 단일 텍스트 리뷰를 입력으로 받아들이면 토큰화된 단어의 ID를 반환\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))\n",
    "\n",
    "\n",
    "# 실제로 입력 데이터셋의 모든 리뷰를 토큰화\n",
    "tokenized_reviews = [tokenize_reviews(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 21), dtype=int32, numpy=\n",
       " array([[ 2054,  5896,  2054,  2466,  2054,  6752,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 3078,  5436,  3078,  3257,  3532,  7613,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 3191,  1996,  2338,  5293,  1996,  3185,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2062, 23873,  3993,  2062, 11259,  2172,  2172,  2062, 14888,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2023,  3185,  2003,  6659,  2021,  2009,  2038,  2070,  2204,\n",
       "          3896,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 1045,  2876,  9278,  2023,  2028,  2130,  2006,  7922, 12635,\n",
       "          2305,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 8235,  1998,  3048,  4616,  2011,  3419,  2457, 27727,  1998,\n",
       "          2848, 16133,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 1045,  3246,  2023,  2177,  1997,  2143, 11153,  2196,  2128,\n",
       "         15908,  2015,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 7918, 14674,  7662,  2003,  6581,  2003,  2023,  2143,  2002,\n",
       "          3084, 17160,  2450,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [11861,  1996, 21442,  6895,  3238,  2515,  2210, 22759,  6198,\n",
       "          1998,  3185,  2087, 12487,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2023,  2003,  2307,  3185,  2205,  2919,  2009,  2003,  2025,\n",
       "          2800,  2006,  2188,  2678,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2017,  2488,  5454,  2703,  2310, 25032,  8913,  8159,  2130,\n",
       "          2065,  2017,  2031,  3427,  2009,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2053,  7615,  5236,  3185,  3772,  2779,  2030,  4788,  9000,\n",
       "          2053,  3168,  2012,  2035, 13558,  2009,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 1045,  2123,  2113,  2339,  2066,  2023,  3185,  2061,  2092,\n",
       "          2021,  2196,  2131,  5458,  1997,  3666,  2009,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 7615,  2023,  3185,  2003,  5263,  2003,  6659,  2200, 17727,\n",
       "          3217,  3676,  3468,  2919,  7613,  3257,  2025,  2298,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2146, 11771,  1038,  8523,  8458,  6633,  3560,  2196,  2031,\n",
       "          2042,  2061,  5580,  2000,  2156,  4566,  6495,  4897,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2074,  2293,  1996,  6970, 13068,  2090,  2048,  2307,  3494,\n",
       "          1997,  2754,  3898,  2310,  3593,  2102,  6287,  5974,     0,\n",
       "             0,     0,     0],\n",
       "        [ 5587,  2023,  2210, 17070,  2000,  2115,  2862,  1997,  6209,\n",
       "         24945,  2009, 26354, 28394,  2102,  6057,  1998,  2203, 27242,\n",
       "             0,     0,     0],\n",
       "        [ 2023,  2003,  1996, 15764,  3185,  2544,  1997,  8429, 24905,\n",
       "         17988,  7659,  2498,  2021,  2045,  2024,  2053, 13842,  5312,\n",
       "             0,     0,     0],\n",
       "        [ 2235,  3077,  2792,  3425,  2003,  1996,  2190,  2792,  1997,\n",
       "          2235,  3077,  2009,  2026,  5440,  2792,  1997,  2235,  3077,\n",
       "             0,     0,     0],\n",
       "        [ 1037,  7244,  3185,  2009,  2003,  2440,  1997,  6699,  1998,\n",
       "          6919,  3772,  2071,  2031,  2938,  2083,  2009,  2117,  2051,\n",
       "             0,     0,     0],\n",
       "        [ 7078, 10392,  3649,  2360,  2876,  2079,  2023,  2104,  9250,\n",
       "          3185,  1996,  3425,  2009, 17210,  3422,  2009,  2085, 10392,\n",
       "             0,     0,     0],\n",
       "        [ 2023,  2003,  2204,  2143,  2023,  2003,  2200,  6057,  2664,\n",
       "          2044,  2023,  2143,  2045,  2020,  2053,  2204,  8471,  3152,\n",
       "             0,     0,     0],\n",
       "        [ 2235,  3077,  2792,  3425,  2003,  1996,  2190,  2792,  1997,\n",
       "          2235,  3077,  2009,  2026,  5440,  2792,  1997,  2235,  3077,\n",
       "             0,     0,     0],\n",
       "        [ 1037,  5790,  1997,  2515,  2025,  4088,  2000,  4671,  2129,\n",
       "         10634,  2139, 24128,  1998, 21660,  2135,  2919,  2023,  3185,\n",
       "          2003,     0,     0],\n",
       "        [ 6283,  2009,  2007,  2035,  2026,  2108,  5409,  3185,  2412,\n",
       "         10597, 21985,  2393,  2033,  2009,  2001,  2008,  2919,  3404,\n",
       "          2033,     0,     0],\n",
       "        [ 1037,  2033,  6491, 11124,  6774,  2143,  2008,  5121,  7906,\n",
       "          2115,  3086,  3841, 13196,  2003, 17160,  1998, 26103,  2000,\n",
       "          3422,     0,     0],\n",
       "        [ 7244,  2092,  2856, 10828,  1997, 10904,  2402,  2472,  3135,\n",
       "          2293,  2466,  2007, 10958,  8428, 10102,  1999,  1996,  4281,\n",
       "          4276,  3773,     0],\n",
       "        [ 2005,  5760,  7788,  4393,  8808,  2498,  2064, 12826,  2000,\n",
       "          1996, 11056,  3152,  3811, 16755,  2169,  1998,  2296,  2028,\n",
       "          1997,  2068,     0],\n",
       "        [ 2028,  1997,  1996,  4569, 15580,  2102,  5691,  2081,  1999,\n",
       "          3522,  2086,  2204, 23191,  5436,  1998, 11813,  6370,  2191,\n",
       "          2023,  2028,  4438],\n",
       "        [ 2023,  3185,  2097,  2467,  2022,  5934,  1998,  3185,  4438,\n",
       "          2004,  2146,  2004,  2045,  2024,  2145,  2111,  2040,  6170,\n",
       "          3153,  1998,  2552],\n",
       "        [ 2023,  2003,  6659,  3185,  2123,  5949,  2115,  2769,  2006,\n",
       "          2009,  2123,  2130,  3422,  2009,  2005,  2489,  2008,  2035,\n",
       "          2031,  2000,  2360]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 1, 1, 1, 1, 1, 0], dtype=int32)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 코드 10-51 문장 길이 통일\n",
    "import random\n",
    "\n",
    "# 토큰화된 리뷰, 리뷰 레이블, 리뷰 길이가 포함된 리스트(list)를 생성\n",
    "reviews_with_len = [\n",
    "    [review, y[i], len(review)] for i, review in enumerate(tokenized_reviews)\n",
    "]\n",
    "random.shuffle(reviews_with_len)\n",
    "# sort( ) 메서드를 사용하여 리뷰를 기준으로 데이터를 정렬\n",
    "reviews_with_len.sort(key=lambda x: x[2])\n",
    "sorted_reviews_labels = [\n",
    "    (review_lab[0], review_lab[1]) for review_lab in reviews_with_len\n",
    "]\n",
    "# sorted_reviews_labels, output_types에 대한 결과를 int32 형식으로 출력\n",
    "processed_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32)\n",
    ")\n",
    "# 배치 크기를 32로 설정, 즉 리뷰 32건을 처리한 후 신경망의 가중치를 업데이트\n",
    "BATCH_SIZE = 32\n",
    "# 데이터셋에 패딩을 적용\n",
    "batched_dataset = processed_dataset.padded_batch(\n",
    "    BATCH_SIZE, padded_shapes=((None,), ())\n",
    ")\n",
    "\n",
    "next(iter(batched_dataset))  # 첫 번째 배치를 출력하고 패딩이 어떻게 적용되었는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-52 데이터셋을 훈련과 검증 세트로 분리\n",
    "import math\n",
    "\n",
    "# 전체 레코드를 32(배치 크기)로 나누어 줌으로써 전체 배치 크기를 구합니다.\n",
    "TOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) / BATCH_SIZE)\n",
    "# 데이터의 10%는 검증을 위해 남겨 둡니다.\n",
    "TEST_BATCHES = TOTAL_BATCHES // 10\n",
    "batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "\n",
    "# test_data에 데이터를 저장하려고 batched_dataset( ) 객체의 take( ) 메서드를 사용합니다.\n",
    "test_data = batched_dataset.take(TEST_BATCHES)\n",
    "# 나머지 데이터는 skip( ) 메서드를 사용하여 훈련을 위해 train_data에 저장합니다.\n",
    "train_data = batched_dataset.skip(TEST_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabulary_file,\n",
    "        embedding_dimnesions=128,\n",
    "        cnn_filters=50,\n",
    "        dnn_units=512,\n",
    "        model_output_classes=2,\n",
    "        dropout_rate=0.1,\n",
    "        training=False,\n",
    "        name=\"text_model\",\n",
    "    ):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocabulary_file, embedding_dimnesions\n",
    "        )\n",
    "        self.cnn_layer1 = tf.keras.layers.Conv1D(\n",
    "            filters=cnn_filters, kernel_size=2, padding=\"valid\", activation=\"relu\"\n",
    "        )\n",
    "        self.cnn_layer2 = tf.keras.layers.Conv1D(\n",
    "            filters=cnn_filters, kernel_size=3, padding=\"valid\", activation=\"relu\"\n",
    "        )\n",
    "        self.cnn_layer3 = tf.keras.layers.Conv1D(\n",
    "            filters=cnn_filters, kernel_size=4, padding=\"valid\", activation=\"relu\"\n",
    "        )\n",
    "        # 합성곱 신경망 계층 세 개가 각각 커널 또는 필터 값(2, 3, 4)으로 초기화되었습니다. 이때 원한다면 필터 크기를 변경해 볼 수 있습니다.\n",
    "        self.pool = tf.keras.layers.GlobalMaxPool1D()\n",
    "        self.dense_1 = tf.keras.layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        # 10% 비율에 대해 드롭아웃 (deactivation)을 적용합니다.\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = tf.keras.layers.Dense(units=1, activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = tf.keras.layers.Dense(\n",
    "                units=model_output_classes, activation=\"softmax\"\n",
    "            )\n",
    "\n",
    "    # 함수를 호출하는 것처럼 클래스의 객체도 호출할 수 있게 만들 수 있는데, 이때 필요한 메서드가 __call__입니다.\n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l)\n",
    "        l_1 = self.pool(l_1)\n",
    "        l_2 = self.cnn_layer2(l)\n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        # call() 함수 내에서 각 합성곱 신경망 계층의 출력에 전역 최대 풀링을 적용합니다.\n",
    "        l_3 = self.pool(l_3)\n",
    "\n",
    "        # 합성곱층 세 개가 함께 연결(concat)되고 그 출력이 첫 번째 신경망에 공급됩니다.\n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1)\n",
    "        # 두 번째로 연결된 신경망은 클래스 두 개만 포함하므로 출력 감정을 예측하는 데 사용됩니다.\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training=training)\n",
    "        # 모델의 출력층\n",
    "        model_output = self.last_dense(concatenated)\n",
    "\n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-54 하이퍼파라미터 초기화\n",
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-55 하이퍼파라미터 값을 네트워크에 전달\n",
    "text_model = TEXT_MODEL(\n",
    "    vocabulary_file=VOCAB_LENGTH,\n",
    "    embedding_dimnesions=EMB_DIM,\n",
    "    cnn_filters=CNN_FILTERS,\n",
    "    dnn_units=DNN_UNITS,\n",
    "    model_output_classes=OUTPUT_CLASSES,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 0.3017 - accuracy: 0.8667\n",
      "Epoch 2/5\n",
      "1407/1407 [==============================] - 60s 43ms/step - loss: 0.1333 - accuracy: 0.9521\n",
      "Epoch 3/5\n",
      "1407/1407 [==============================] - 60s 43ms/step - loss: 0.0710 - accuracy: 0.9749\n",
      "Epoch 4/5\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 0.0432 - accuracy: 0.9846\n",
      "Epoch 5/5\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 0.0221 - accuracy: 0.9923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efc2085de80>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 코드 10-56 모델 훈련\n",
    "if OUTPUT_CLASSES == 2:\n",
    "    text_model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "else:\n",
    "    text_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "\n",
    "text_model.fit(train_data, epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 1s 7ms/step - loss: 0.4391 - accuracy: 0.8968\n",
      "[0.43907997012138367, 0.8968349099159241]\n"
     ]
    }
   ],
   "source": [
    "# 코드 10-57 모델 성능 평가\n",
    "results = text_model.evaluate(test_data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-58 라이브러리 호출\n",
    "import pandas as pd\n",
    "import bert\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-59 데이터셋 메모리로 로딩\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "test_data = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-60 텍스트 토큰화\n",
    "url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/2\"\n",
    "bert_layer = hub.KerasLayer(url, trainable=True)\n",
    "\n",
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 10-61 텍스트 전처리\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def bert_encoder(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "\n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)  # 입력 데이터를 토큰으로 변환\n",
    "        text = text[: max_len - 2]\n",
    "        # CLS/SEP 처리(버트에서 입력 값에 대한 임베딩을 위한 식별자)\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        # 제로 패딩 적용\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "\n",
    "train_input = bert_encoder(train_data, tokenizer, max_len=160)\n",
    "train_labels = train_data.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segment_ids (InputLayer)  [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 input_segment_ids[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            1025        keras_layer_1[1][0]              \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 코드 10-62 모델 생성\n",
    "def build_model(max_len=512):\n",
    "    # 입력층에서 사용할 input_word_ids 정의\n",
    "    input_word_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\"\n",
    "    )\n",
    "    # 입력층에서 사용할 input_segment_ids 정의\n",
    "    input_segment_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"input_segment_ids\"\n",
    "    )\n",
    "    # 입력층에서 사용할 input_mask 정의\n",
    "    input_mask = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"input_mask\"\n",
    "    )\n",
    "    # 버트 계층의 입력 값으로 앞서 정의한 input_word_ids, input_segment_ids, input_mask 사용\n",
    "    pooled_output, sequence_output = bert_layer(\n",
    "        [input_word_ids, input_mask, input_segment_ids]\n",
    "    )\n",
    "    clf_output = pooled_output\n",
    "    # 출력층(완전연결층) 정의\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(clf_output)\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_word_ids, input_mask, input_segment_ids], outputs=output\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=RMSprop(learning_rate=2e-6), loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_model(max_len=160)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3820 - accuracy: 0.7500 - val_loss: 0.0614 - val_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 0.0996 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 0.1760 - accuracy: 1.0000 - val_loss: 0.0485 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 코드 10-63 모델 훈련\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels, validation_split=0.2, epochs=3, batch_size=8\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
