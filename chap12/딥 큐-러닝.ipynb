{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting progressbar2\n",
      "  Downloading progressbar2-4.5.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting python-utils>=3.8.1 (from progressbar2)\n",
      "  Downloading python_utils-3.8.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: typing-extensions>3.10.0.2 in /home/halozhan/Study/.venv/lib/python3.8/site-packages (from python-utils>=3.8.1->progressbar2) (4.12.2)\n",
      "Downloading progressbar2-4.5.0-py3-none-any.whl (57 kB)\n",
      "Downloading python_utils-3.8.2-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: python-utils, progressbar2\n",
      "Successfully installed progressbar2-4.5.0 python-utils-3.8.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /home/halozhan/Study/.venv/lib/python3.8/site-packages (from gym) (1.18.5)\n",
      "Collecting cloudpickle>=1.2.0 (from gym)\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gym-notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/halozhan/Study/.venv/lib/python3.8/site-packages (from gym) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/halozhan/Study/.venv/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym) (3.20.2)\n",
      "Downloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827621 sha256=199be7e578489f6a6699c8264776071e31c73811a365c614cb3320ae7170c535\n",
      "  Stored in directory: /home/halozhan/.cache/pip/wheels/17/79/65/7afedc162d858b02708a3b8f7a6dd5b1000dcd5b0f894f7cc1\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-3.1.0 gym-0.26.2 gym-notices-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install progressbar2\n",
    "%pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 15:13:04.774146: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2024-12-03 15:13:04.774187: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# 코드 12-1 라이브러리 호출\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from collections import deque  # 에이전트가 환경에 반응한 경험을 저장\n",
    "import gym\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Reshape\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym[toy_text] in /home/halozhan/.local/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/halozhan/.local/lib/python3.10/site-packages (from gym[toy_text]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/halozhan/.local/lib/python3.10/site-packages (from gym[toy_text]) (3.1.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/halozhan/.local/lib/python3.10/site-packages (from gym[toy_text]) (0.0.8)\n",
      "Requirement already satisfied: pygame==2.1.0 in /home/halozhan/.local/lib/python3.10/site-packages (from gym[toy_text]) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym[toy_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "\n",
      "취할 수 있는 상태 수: 500\n",
      "취할 수 있는 행동 수: 6\n"
     ]
    }
   ],
   "source": [
    "# 코드 12-2 ‘Taxi-v3’에 대한 환경\n",
    "# Taxi-v3 객체를 인스턴스로 생성\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"ansi\")\n",
    "env.reset()\n",
    "\n",
    "# 환경 및 에이전트의 현재 상태를 render 메서드를 사용하여 표시\n",
    "print(env.render())\n",
    "\n",
    "print('취할 수 있는 상태 수: {}'.format(env.observation_space.n)) # 환경의 모든 상태에 접근\n",
    "print('취할 수 있는 행동 수: {}'.format(env.action_space.n)) # 환경의 모든 행동에 접근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 12-3 에이전트 구현\n",
    "class Agent:\n",
    "    def __init__(self, env, optimizer):  # 상태와 행동을 초기화\n",
    "        self._state_size = (\n",
    "            env.observation_space.n\n",
    "        )  # 환경 속성 observation_space를 초기화\n",
    "        self._action_size = env.action_space.n  # 환경 속성 action_space를 초기화\n",
    "        self._optimizer = optimizer  # 옵티마이저 초기화\n",
    "        self.expirience_replay = deque(maxlen=2000)  # 과거 행동에 대한 기억을 초기화\n",
    "        self.gamma = 0.6  # 할인율 초기화\n",
    "        self.epsilon = 0.1  # 탐험 비율 초기화\n",
    "\n",
    "        self.q_network = (\n",
    "            self.build_compile()\n",
    "        )  # build_compile() 함수를 사용하여 큐-네트워크 구성\n",
    "        self.target_network = (\n",
    "            self.build_compile()\n",
    "        )  # build_compile() 함수를 사용하여 타깃 큐-네트워크 구성\n",
    "        self.target_model()\n",
    "\n",
    "    def store(self, state, action, reward, next_state, terminated):\n",
    "        self.expirience_replay.append((state, action, reward, next_state, terminated))\n",
    "\n",
    "    def build_compile(self):  # 네트워크 구성을 위한 함수\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(self._state_size, 10, input_length=1))\n",
    "        model.add(Reshape((10,)))\n",
    "        model.add(Dense(50, activation=\"relu\"))\n",
    "        model.add(Dense(50, activation=\"relu\"))\n",
    "        model.add(Dense(50, activation=\"relu\"))\n",
    "        model.add(Dense(self._action_size, activation=\"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer=self._optimizer)\n",
    "        return model\n",
    "\n",
    "    def target_model(self):  # 가중치를 적용하기 위한 함수\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "    def act(self, state):  # 탐험을 위한 함수\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        q_values = self.q_network.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def retrain(self, batch_size):  # 큐-네트워크 훈련에 대한 함수\n",
    "        # 리플레이 메모리에서 랜덤한 데이터 선택\n",
    "        minibatch = random.sample(self.expirience_replay, batch_size)\n",
    "        target = self.q_network.predict(state)\n",
    "        if terminated:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            t = self.target_network.predict(next_state)\n",
    "            target[0][action] = reward + self.gamma * np.amax(t)\n",
    "        self.q_network.fit(state, target, epochs=1, verbose=0)  # 큐-네트워크 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1, 10)             5000      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 10,956\n",
      "Trainable params: 10,956\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 15:13:18.990366: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2024-12-03 15:13:19.152847: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-03 15:13:19.152876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:0a:00.0 name: NVIDIA GeForce GTX 1080 computeCapability: 6.1\n",
      "coreClock: 1.759GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\n",
      "2024-12-03 15:13:19.152943: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2024-12-03 15:13:19.152988: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\n",
      "2024-12-03 15:13:19.153023: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2024-12-03 15:13:19.153057: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2024-12-03 15:13:19.153088: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
      "2024-12-03 15:13:19.153119: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\n",
      "2024-12-03 15:13:19.153151: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-03 15:13:19.153154: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-12-03 15:13:19.153472: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-03 15:13:19.162384: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3800105000 Hz\n",
      "2024-12-03 15:13:19.163660: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x24303b30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-12-03 15:13:19.163672: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-12-03 15:13:19.164757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-12-03 15:13:19.164768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      \n"
     ]
    }
   ],
   "source": [
    "# 코드 12-4 훈련 준비\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "agent = Agent(env, optimizer)\n",
    "batch_size = 32\n",
    "num_of_episodes = 10\n",
    "timesteps_per_episode = 10\n",
    "agent.q_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: (228, {'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)})\n",
      "Step result: (328, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 328\n",
      "Step result: (428, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 428\n",
      "Step result: (428, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 428\n",
      "Step result: (428, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 428\n",
      "Step result: (428, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 428\n",
      "Step result: (428, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 428\n",
      "Step result: (428, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 428\n",
      "Step result: (428, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 428\n",
      "Step result: (428, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 428\n",
      "Step result: (428, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 428\n",
      "Initial state: (434, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Step result: (434, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 434\n",
      "Step result: (434, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 434\n",
      "Step result: (434, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 434\n",
      "Step result: (434, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 434\n",
      "Step result: (434, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 434\n",
      "Step result: (434, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 434\n",
      "Step result: (434, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 434\n",
      "Step result: (434, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 434\n",
      "Step result: (434, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 434\n",
      "Step result: (434, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 434\n",
      "Initial state: (342, {'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Step result: (442, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 442\n",
      "Step result: (442, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 442\n",
      "Step result: (442, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 442\n",
      "Step result: (442, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 442\n",
      "Step result: (442, -10, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 442\n",
      "Step result: (422, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 422\n",
      "Step result: (422, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 422\n",
      "Step result: (422, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 422\n",
      "Step result: (422, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 422\n",
      "Step result: (422, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 422\n",
      "Initial state: (306, {'prob': 1.0, 'action_mask': array([1, 1, 0, 0, 0, 0], dtype=int8)})\n",
      "Step result: (406, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 0, 0, 0], dtype=int8)})\n",
      "Next state: 406\n",
      "Step result: (406, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 0, 0, 0], dtype=int8)})\n",
      "Next state: 406\n",
      "Step result: (406, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 0, 0, 0], dtype=int8)})\n",
      "Next state: 406\n",
      "Step result: (406, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 0, 0, 0], dtype=int8)})\n",
      "Next state: 406\n",
      "Step result: (406, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 0, 0, 0], dtype=int8)})\n",
      "Next state: 406\n",
      "Step result: (406, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 0, 0, 0], dtype=int8)})\n",
      "Next state: 406\n",
      "Step result: (406, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 0, 0, 0], dtype=int8)})\n",
      "Next state: 406\n",
      "Step result: (406, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 0, 0, 0], dtype=int8)})\n",
      "Next state: 406\n",
      "Step result: (406, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 0, 0, 0], dtype=int8)})\n",
      "Next state: 406\n",
      "Step result: (406, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 0, 0, 0], dtype=int8)})\n",
      "Next state: 406\n",
      "Initial state: (164, {'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)})\n",
      "Step result: (264, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)})\n",
      "Next state: 264\n",
      "Step result: (364, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 364\n",
      "Step result: (464, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 464\n",
      "Step result: (464, -10, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 464\n",
      "Step result: (464, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 464\n",
      "Step result: (464, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 464\n",
      "Step result: (464, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 464\n",
      "Step result: (464, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 464\n",
      "Step result: (464, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 464\n",
      "Step result: (464, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 464\n",
      "Initial state: (441, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Step result: (441, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 441\n",
      "Step result: (441, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 441\n",
      "Step result: (441, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 441\n",
      "Step result: (441, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 441\n",
      "Step result: (441, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 441\n",
      "Step result: (441, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 441\n",
      "Step result: (441, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 441\n",
      "Step result: (421, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 421\n",
      "Step result: (421, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 421\n",
      "Step result: (421, -1, False, False, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 421\n",
      "Initial state: (44, {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)})\n",
      "Step result: (64, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 1, 1, 0, 0], dtype=int8)})\n",
      "Next state: 64\n",
      "Step result: (84, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 1, 0], dtype=int8)})\n",
      "Next state: 84\n",
      "Step result: (84, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 1, 0], dtype=int8)})\n",
      "Next state: 84\n",
      "Step result: (84, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 1, 0], dtype=int8)})\n",
      "Next state: 84\n",
      "Step result: (84, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 1, 0], dtype=int8)})\n",
      "Next state: 84\n",
      "Step result: (84, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 1, 0], dtype=int8)})\n",
      "Next state: 84\n",
      "Step result: (64, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 1, 1, 0, 0], dtype=int8)})\n",
      "Next state: 64\n",
      "Step result: (44, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 44\n",
      "Step result: (44, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 44\n",
      "Step result: (44, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 44\n",
      "Initial state: (152, {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Step result: (152, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 152\n",
      "Step result: (152, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 152\n",
      "Step result: (52, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 52\n",
      "Step result: (52, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 52\n",
      "Step result: (52, -10, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 52\n",
      "Step result: (52, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 52\n",
      "Step result: (52, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 52\n",
      "Step result: (52, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 52\n",
      "Step result: (52, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 52\n",
      "Step result: (52, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 52\n",
      "Initial state: (83, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)})\n",
      "Step result: (83, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 83\n",
      "Step result: (83, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 83\n",
      "Step result: (83, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 83\n",
      "Step result: (83, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 83\n",
      "Step result: (83, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 83\n",
      "Step result: (83, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 83\n",
      "Step result: (83, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 83\n",
      "Step result: (83, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 83\n",
      "Step result: (83, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 83\n",
      "Step result: (83, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 83\n",
      "Initial state: (173, {'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)})\n",
      "Step result: (193, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 193\n",
      "Step result: (193, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 193\n",
      "Step result: (193, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Next state: 193\n",
      "Step result: (173, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)})\n",
      "Next state: 173\n",
      "Step result: (153, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 153\n",
      "Step result: (153, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 153\n",
      "Step result: (153, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 153\n",
      "Step result: (153, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 153\n",
      "Step result: (153, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 153\n",
      "Step result: (153, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Next state: 153\n",
      "**********************************\n",
      "Episode: 10\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : |\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "**********************************\n"
     ]
    }
   ],
   "source": [
    "# 코드 12-5 모델 훈련\n",
    "for e in range(0, num_of_episodes):\n",
    "    state = env.reset()  # 환경 재설정\n",
    "    print(f\"Initial state: {state}\")  # state의 구조 확인\n",
    "    state = (\n",
    "        np.array([state[0]]).reshape([1, -1]).astype(np.float32)\n",
    "    )  # 첫 번째 요소만 사용하여 배열로 변환\n",
    "\n",
    "    reward = 0  # 보상 변수 초기화\n",
    "    terminated = False\n",
    "\n",
    "    for timestep in range(timesteps_per_episode):\n",
    "        action = agent.act(state)  # act() 함수 실행\n",
    "\n",
    "        # 에이전트가 단계별 행동을 취합니다.\n",
    "        result = env.step(action)\n",
    "        print(f\"Step result: {result}\")  # step 결과의 구조 확인\n",
    "        next_state, reward, terminated, truncated, info = result\n",
    "        print(f\"Next state: {next_state}\")  # next_state의 구조 확인\n",
    "        # 첫 번째 요소만 사용하여 배열로 변환\n",
    "        next_state = np.array([next_state]).reshape([1, -1]).astype(np.float32)\n",
    "        agent.store(state, action, reward, next_state, terminated)\n",
    "        state = next_state\n",
    "\n",
    "        if terminated:\n",
    "            agent.target_model()\n",
    "            break\n",
    "\n",
    "        if len(agent.expirience_replay) > batch_size:\n",
    "            agent.retrain(batch_size)\n",
    "\n",
    "    if (e + 1) % 10 == 0:\n",
    "        print(\"**********************************\")\n",
    "        print(\"Episode: {}\".format(e + 1))\n",
    "        print(env.render())\n",
    "        print(\"**********************************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
